<!DOCTYPE html>
<html>
<head>
  <title>表情管理アプリケーション</title>
  <!-- 最新バージョンのChart.jsを読み込む -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
  <!-- カメラの映像を表示するためのvideo要素 -->
  <video id="video" width="640" height="480" autoplay></video>
  <!-- グラフを表示するためのCanvas要素 -->
  <canvas id="emotionChart" width="400" height="200"></canvas>
  <div id="result"></div>

  <script>
    // Azure Face APIのエンドポイントとサブスクリプションキー
    const endpoint = 'https://test22222.cognitiveservices.azure.com/face/v1.0/detect';
    const apiKey = 'f0174e3e4fe94a098952d53baf97d8ee'; // ここにEmotion APIのキーを置き換える

    // Chart.jsを初期化
    const ctxChart = document.getElementById('emotionChart').getContext('2d');
    const emotionChart = new Chart(ctxChart, {
      type: 'bar',
      data: {
        labels: ['笑顔', '悲しい顔'],
        datasets: [{
          label: '表情スコア',
          data: [0, 0], // 初期値は0
          backgroundColor: [
            'rgba(75, 192, 192, 0.2)',
            'rgba(255, 99, 132, 0.2)',
          ],
          borderColor: [
            'rgba(75, 192, 192, 1)',
            'rgba(255, 99, 132, 1)',
          ],
          borderWidth: 1
        }]
      },
      options: {
        scales: {
          y: {
            beginAtZero: true
          }
        }
      }
    });

    // カメラの起動ボタンを追加し、カメラの使用許可を得る関数
    function startCamera() {
      const video = document.getElementById('video');

      // カメラの使用許可を取得
      navigator.mediaDevices.getUserMedia({ video: true })
        .then((stream) => {
          video.srcObject = stream;
          video.play(); // 映像の再生を開始
          detectEmotion(video);
        })
        .catch((error) => {
          console.error('Webカメラの利用許可がありません。', error);
        });
    }

    // フレームごとに表情を取得する関数
    function detectEmotion(video) {
      const canvas = document.createElement('canvas');
      canvas.width = video.width;
      canvas.height = video.height;
      const ctx = canvas.getContext('2d');
      const resultDiv = document.getElementById('result');

      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      const image = canvas.toDataURL('image/jpeg');

      // Face APIにリクエストを送信
      fetch(endpoint + '?returnFaceAttributes=emotion', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Ocp-Apim-Subscription-Key': apiKey,
        },
        body: JSON.stringify({ url: image }),
      })
      .then((response) => response.json())
      .then((data) => {
        if (data && data.length > 0) {
          const emotions = data[0].faceAttributes.emotion;
          const { happiness, sadness } = emotions;

          // グラフを更新して表情スコアを表示
          emotionChart.data.datasets[0].data = [happiness, sadness];
          emotionChart.update();

          // 表情スコアをテキストとして表示
          resultDiv.innerHTML = `
            <p>笑顔: ${happiness}</p>
            <p>悲しい顔: ${sadness}</p>
          `;
        }
      })
      .catch((error) => {
        console.error('Face APIのリクエストに失敗しました。', error);
      });

      // 1000msごとに表情を取得
      setTimeout(() => detectEmotion(video), 1000);
    }

    // カメラの起動ボタンをクリックしてカメラの使用許可を得る
    const startButton = document.createElement('button');
    startButton.textContent = 'カメラを起動する';
    startButton.addEventListener('click', () => startCamera());
    document.body.appendChild(startButton);
  </script>
</body>
</html>
